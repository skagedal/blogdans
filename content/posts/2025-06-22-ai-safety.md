---
layout: post
title: "On AI Safety"
summary: ""
draft: true
---

You may have headlines about how generative AI is going to take our jobs. Or, at least, some jobs. Some examples from here in Sweden: [this article](https://www.dn.se/ekonomi/rapport-300-000-jobb-kan-helt-eller-delvis-ersattas-av-ai-i-sverige/) from Dagens Nyheter last year, based on a report from Google, warns about 300 000 Swedish jobs being fully or partially replaced by AI – 6 % of our jobs. A year later, [a Dagens Nyheter article](https://www.dn.se/ekonomi/rapport-har-ar-jobben-som-kan-ersattas-av-ai/) presents a report that warns that the following jobs might be threatened: graphic designer, accountant, statistician, medical secretary, administrative assistant, computer programmer, software developer, sound engineer. 

I'm a computer programmer. Or a software developer. As far as I'm aware, those are the same thing. So, am I worried? 

Yes, I am worried. But maybe not in the way reflected by these articles. 

First off, one thing that's odd about the premise of the "AIs will take our jobs" story is that it seems to be assuming a zero-sum game, where there is a fixed number of software developer-shaped holes that the AI would fill. One is reminded of how other inventions – such as the personal computer itself – historically have been discussed upon their introduction, how there would no longer be a need for certain administrative jobs. As a threat or a promise, like when Steve Jobs envisioned the [office without a secretary](https://www.cnbc.com/2019/10/17/steve-jobs-vision-for-the-office-of-the-future-in-the-1980s.html). 

But, of course, we seem to have [more](https://www.bls.gov/opub/ted/2024/18-5-million-office-and-administrative-support-jobs-in-may-2023-12-2-percent-of-total-employment.htm) administrative tasks to do than ever. This type of historical comparison has been used (by who?) to argue that we similarly don't need to woryr about AI taking our jobs – it will create new jobs that we don't even know about yet. In the near term, I agree with this. 

Similarly, I agree with critics that the level of AI-assisted programming today is – in a sense – nowhere near being able to take my job. "Nowhere near"? I hope you noted the "in a sense" clause in that sentence, because it does quite a bit of heavy lifting. I'm afraid you're going to have to read the rest of the blog post to learn why in what sense I think it is true, and in what sense I think it is, in fact, bananas. 

---

So, I use these tools daily, and they certainly help speed up my work. As a software developer, one part of my job is writing code. It is far from the only part, but it is the thing I do which the whole thing sort of is about, you know. So let's look a little bit at that.

Back when I started programming, back in the 1980s, the programmer wrote the code. That was pretty much it. I'm not sure how much you, dear reader, know about computer programming – I don't even know who you are, so please leave a comment! – but for the sake of discussion I thought we would look at some very simple, and simplified, code together. 

```typescript
// This class groups together some behavior related to bananas
class Banana {
   function eat() {
      // Here goes code related to eating a banana
   };
}

class Flower {
	function smell() {
	   // Here goes code related to smelling a flower
	}
}

class Garden {
   function pickBanana(): Banana {
       // And here we have some code that picks a banana from the garden, 
       // returning it to the caller
   }
   function pickFlower(): Flower {
       // Similarly, this one picks a flower and returns it to the caller.
   };
}
```

So what this means is that we have some kind of object that represents a "garden", and from that garden you can either pick a banana or a flower. Bananas only have one action ("method" in object-oriented programming lingo) you can do on them, which is "eat", and flowers can only be smelled. So – assuming that you already have access to a `garden`, but nothing else – the programmer might write: `garden.pickBanana().eat()` and then something related to eating bananas happen. 

Then the IDEs – the integrated development environments, the tools in which we write code – got smarter, and "auto-complete" was introduced. Somewhere around 20–30 years ago. In such an environment the programmer might type just the letter `g` – and the IDE would helpfully suggest a _completion_, namely `garden`. You then continue to type `.pickB` and immediately the IDE goes "i know, i know! This is going to be banana!" – and fills it in for you. 

But it knows nothing else beyond the structural components it's been told about. Lots of other such smartness were added to IDEs over the years, making it more convenient to rename things in the code, various operations to "refactor" code – but all operating on the well-defined structured syntax of the programming language in question.

This changed with the debut of Github Copilot in 2021, which was the first widely available AI-assisted code completion tool. Now it would understand natural language. Maybe I'd start implementing a function to feed the monkey:

```typescript
function feedMonkey() {
```

...and the AI would suggest the implementation...  

```typescript
   garden.pickBanana().eat();
}
```

The AI here assumes that we should pick a banana and eat it, rather than a flower to smell, because we're feeding a monkey. Monkeys like bananas. 

It's worth taking a moment to compare the two kinds of intelligences here: the structural auto-complete one and the AI-assisted one. (And yes, features like auto-complete have often be referred to in terms of intelligence – "IntelliSense" has been the Microsoft brand name for such features for a long time.) The first one is intelligent in the way that we have been accustomed to think of computers as intelligent: it does exactly what it's been programmed to do. It does not make mistakes (unless there's a bug). It _knows_ that the `garden` object has exactly these two capabilities – picking bananas and flowers – whereas the AI assistant might suddenly try to invoke a non-existing `pickApple()` method. 

The AI-assisted intelligence is a more human-like intelligence. Like humans, it makes mistakes – sometimes hilarious ones. But also, like humans, it can also do a lot more than the structural auto-complete, and other tools of that paradigm. It can implement whole methods, classes, even programs. 

I am personally using AI tools like Github Copilot, ChatGPT and Claude to generate code, research programming problems, write documentation and a lot of other things. I am regularly blown away by how far it has gotten, and it often  provides me with a large chunk of code or text that was just what I wanted. But still, to put all the things that make up my job as a software developer together, there is a lot left. 

Yeah, let's get back to that. Remember how I said a few paragraphs back that the AIs of today are "nowhere near" taking my job. What does that sentence even mean? What does "nowhere near" mean? Terms like "near" and "far" – these are relative measures of some dimension that I haven't even specified. What am I talking about? Nowhere near in time? In space? In IQ points? In spent dollars? 

Sorry, we have to go off on yet another weird tangent for a moment.

Imagine that a group of scientists suddenly discover a new species of intelligent apes. Somehow, they had been hiding all this time in a distant part of the Amazonian jungle. They are somewhat smarter than chimpanzees and have developed language skills, but they are not as good at general reasoning as us humans. Of course, they have not had access to modern tools so the culture shock is huge, but it turns out that with some training, they can be taught to be useful coding assistants. 

I say banana, and the ape goes: "eat"! I say flower, and the ape goes: "smell!" It's on the level of basic IDE auto-complete, it seems. We call them "chumpanzees". 

Amazingly, we find yet another unknown species of intelligent apes, this time in the Congo basin. They are a bit more advanced than the chumpanzees, and they can do a bit more than just auto-complete. They can write some code, and they can even understand some of the context of what they're doing. They can write some code given detailed feature requirements, and they can even help with trouble-shooting. Let's say they can be compared to the level of 2023 AIs.

But that seems to be about as far as they can go. That seems to be about where they hit their biological limits. For generations, the chumpanzees and champanzees could be helpful to humans in saving them from keyboard typing, but they're not taking our jobs. 

So, in capabilities, they are "nowhere near" my level of skill as a software developer. They can't independently figure out what users need, make a longer term plan on how to implement it and execute on the plan. 

The time it would take for natural selection to develop chumpanzees into champanzees, or the champanzees into a species that could take my job, is a long one. I'm not a biologist, and I'm not sure this is even a sensible question to ask, but I bet the time scales involved for either of those would be on the order of millions, or at least hundreds of thousands, of years.

Fill in these paragraphs:

- Yet, it took just decades for our tools to evolve from the structural auto-complete to the AI-assisted code completion. 
- It took just years from GPT 3.5 to o3. The difference in capabilities is already staggering – if we continue with our ape analogy, this would be yet a whole new species of ape. 
- So the speed of development is accellerating. There is little reason to believe that this will slow down. Enormous amounts of resources are being poured into this area. The companies have a clear goal: develop AGI, artificial general intelligence. Is it "just hype"? I don't think so. 
- So the gap in capabilities still required to take my job could still (1 juni 2025) be described as "nowhere near" – but the time scales involved are so completely different from the time scales of evolution – AND the time scales we have gotten used to in teh innovation.
- Refer to this guide: https://80000hours.org/agi/guide/when-will-agi-arrive/


---
So what is it that I worry about? 

- After AGI, ASI. 
- I am worried about the unknown unknowns that would follow from an imperfectly aligned super-intelligence, and that we don't have much time to prepare for this. That could include the extinction scenario and other dystopias, things that are hard to even imagine. I find it difficult to even imagine. 
- I also think it could lead to amazing new developments

So what could I do about this? 

1. Understand more. Talk and discuss more. 
2. Work to influence policy makers. This requires reaching sufficient mass in the public. Right now it is difficult, I think, to even convince educated groups. A small group of people are aware of this scenario and among those, even fewer take it seriously. Building awareness would have to start there, I think, and for that to happen I think story-telling of various forms are a key component. I hope to contribute to this by blog posts such as this one, but there I so much that I would have to understand better myself first – so it would be a learning-by-doing project. 
3. Help with AI alignment. 
4. Work towards making the process "go well". In many ways this is what companies like OpenAI and Anthropic are doing, or think they are doing – which is then exactly what contributes to the race dynamics involved in what makes all this happen. Yet I think it might be our best shot. This would mean applying AI and LLMs to do go things in the world – I could start where I am now. 
5. Get more involved in cyber-security. Even though I haven't mentioned it explicitly above, I think that perhaps would be an important mediating factor, and it would be something that would feel somewhat "close" to me.

# Log

* 2025-05-14: Went to [[Seminar on AI Safety]] with SCAIS.
* 2025-05-26: Writing session with EA Uppsala. 
* 2025-05-28: Writing session with SCAIS. 
* 2025-06-01: Post a first draft to skagedal.tech.
